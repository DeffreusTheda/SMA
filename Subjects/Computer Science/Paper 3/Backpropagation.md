Backpropagation is an algorithm used in training neural networks to adjust the weights of the network to minimize the prediction error.
It calculates the gradient of the [[loss function]] with [[Gradient Descent|respect]] to each weight by propagating the error backward from the output layer to the input layer.
Used to train sequential neural networks, feedforward neural networks (multi-layer perceptrons, CNNs).

## [[Backpropagation Through Time]]

An extension of the standard backpropagation.

## [[Vanishing Gradient]]