1. Input embedding.
2. Attention block: Vectors exchange information to better represent their meaning in the context of some text.
3. Additional combinations of attention and feedforward layer.
4. Result is rich representation of each token in text, as signified by corresponding vector.
5. Last vector is used to produce probability distribution.